{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c527893c-5f59-4713-a7a4-649a5a19a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import json \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d9893c-0283-4eb0-8b92-54e4fa3ba851",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v8 = YOLO(\"models/yolov8n.pt\")\n",
    "model_v11 = YOLO(\"models/yolo11n.pt\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bc26f5-3a86-41fc-9e12-d2222e16a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_val_path = \"./data/val2017\" \n",
    "coco_ann_path = \"./data/annotations/instances_val2017.json\" \n",
    "\n",
    "coco = COCO(coco_ann_path)\n",
    "image_ids = coco.getImgIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99339fe6-e647-447e-9580-b0fcc7ae8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494618ca-ff95-4d9d-8324-48db0f5bf314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1: List[float], box2: List[float]) -> float:\n",
    "    if len(box1) != 4 or len(box2) != 4:\n",
    "        raise ValueError(\"Box format not supported\")\n",
    "\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    w = max(0, x2 - x1)\n",
    "    h = max(0, y2 - y1)\n",
    "    intersection = w * h\n",
    "\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    union = box1_area + box2_area - intersection\n",
    "\n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b0ab5e-2256-41cc-a64d-ecf1c4882022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model: YOLO, image_ids: list[int]) -> Dict:\n",
    "    results = {}\n",
    "    for idx, img_id in enumerate(image_ids):\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processing image {idx}/{len(image_ids)}\")\n",
    "        \n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(f\"{coco_val_path}/{img_info['file_name']}\")\n",
    "    \n",
    "        try:\n",
    "            prediction = model(img_path, verbose=False)[0]\n",
    "            detections = []\n",
    "            if prediction.boxes is not None: \n",
    "                boxes = prediction.boxes.xyxy.cpu().numpy()\n",
    "                confidences = prediction.boxes.conf.cpu().numpy()\n",
    "                classes = prediction.boxes.cls.cpu().numpy()\n",
    "    \n",
    "                for i in range(len(boxes)):\n",
    "                    detections.append({\n",
    "                        'bbox': boxes[i].tolist(), \n",
    "                        'confidence': float(confidences[i]),\n",
    "                        'class': int(classes[i])\n",
    "                    })\n",
    "    \n",
    "            results[img_id] = detections\n",
    "                \n",
    "        except Exception as e: \n",
    "            print(f\"Error processing image {img_id}: {e}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "482c5a10-49b6-46d7-9640-074a24bfc8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_objects(img_id: int) -> List[Dict]:\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "    gt_objects = []\n",
    "\n",
    "    for ann in anns: \n",
    "        if ann['iscrowd'] == 0: # Only consider non-crowd annotations\n",
    "            # Convert from [x,y,w,h] to [x1,y1,x2,y2]\n",
    "            x, y, w, h = ann['bbox']\n",
    "            bbox = [x, y, x + w , y + h]\n",
    "\n",
    "            gt_objects.append({\n",
    "                'bbox': bbox, \n",
    "                'class': ann['category_id'] - 1,  # COCO classes are 1-indexed, convert to 0-indexed\n",
    "                'area': ann['area']\n",
    "            })\n",
    "    return gt_objects\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77bd9bc-d703-4aca-94ec-186c21d8a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_detections(detections: List[Dict], gt_objects: List[Dict], iou_threshold: float = 0.5) -> Dict: \n",
    "    matches = {}\n",
    "\n",
    "    for gt_idx, gt_obj in enumerate(gt_objects):\n",
    "        best_match = None \n",
    "        best_confidence = -1 \n",
    "\n",
    "        for det in detections:\n",
    "            iou = compute_iou(det['bbox'], gt_obj['bbox'])\n",
    "\n",
    "            if iou >= iou_threshold and det['confidence'] > best_confidence:\n",
    "                best_match = det\n",
    "                best_confidence = det['confidence']\n",
    "\n",
    "        matches[gt_idx] = best_match\n",
    "                \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd4b73ec-b812-4f19-97e2-6d6c20c58856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 0/5000\n",
      "Processing image 100/5000\n",
      "Processing image 200/5000\n",
      "Processing image 300/5000\n",
      "Processing image 400/5000\n",
      "Processing image 500/5000\n",
      "Processing image 600/5000\n",
      "Processing image 700/5000\n",
      "Processing image 800/5000\n",
      "Processing image 900/5000\n",
      "Processing image 1000/5000\n",
      "Processing image 1100/5000\n",
      "Processing image 1200/5000\n",
      "Processing image 1300/5000\n",
      "Processing image 1400/5000\n",
      "Processing image 1500/5000\n",
      "Processing image 1600/5000\n",
      "Processing image 1700/5000\n",
      "Processing image 1800/5000\n",
      "Processing image 1900/5000\n",
      "Processing image 2000/5000\n",
      "Processing image 2100/5000\n",
      "Processing image 2200/5000\n",
      "Processing image 2300/5000\n",
      "Processing image 2400/5000\n",
      "Processing image 2500/5000\n",
      "Processing image 2600/5000\n",
      "Processing image 2700/5000\n",
      "Processing image 2800/5000\n",
      "Processing image 2900/5000\n",
      "Processing image 3000/5000\n",
      "Processing image 3100/5000\n",
      "Processing image 3200/5000\n",
      "Processing image 3300/5000\n",
      "Processing image 3400/5000\n",
      "Processing image 3500/5000\n",
      "Processing image 3600/5000\n",
      "Processing image 3700/5000\n",
      "Processing image 3800/5000\n",
      "Processing image 3900/5000\n",
      "Processing image 4000/5000\n",
      "Processing image 4100/5000\n",
      "Processing image 4200/5000\n",
      "Processing image 4300/5000\n",
      "Processing image 4400/5000\n",
      "Processing image 4500/5000\n",
      "Processing image 4600/5000\n",
      "Processing image 4700/5000\n",
      "Processing image 4800/5000\n",
      "Processing image 4900/5000\n",
      "Processing image 0/5000\n",
      "Processing image 100/5000\n",
      "Processing image 200/5000\n",
      "Processing image 300/5000\n",
      "Processing image 400/5000\n",
      "Processing image 500/5000\n",
      "Processing image 600/5000\n",
      "Processing image 700/5000\n",
      "Processing image 800/5000\n",
      "Processing image 900/5000\n",
      "Processing image 1000/5000\n",
      "Processing image 1100/5000\n",
      "Processing image 1200/5000\n",
      "Processing image 1300/5000\n",
      "Processing image 1400/5000\n",
      "Processing image 1500/5000\n",
      "Processing image 1600/5000\n",
      "Processing image 1700/5000\n",
      "Processing image 1800/5000\n",
      "Processing image 1900/5000\n",
      "Processing image 2000/5000\n",
      "Processing image 2100/5000\n",
      "Processing image 2200/5000\n",
      "Processing image 2300/5000\n",
      "Processing image 2400/5000\n",
      "Processing image 2500/5000\n",
      "Processing image 2600/5000\n",
      "Processing image 2700/5000\n",
      "Processing image 2800/5000\n",
      "Processing image 2900/5000\n",
      "Processing image 3000/5000\n",
      "Processing image 3100/5000\n",
      "Processing image 3200/5000\n",
      "Processing image 3300/5000\n",
      "Processing image 3400/5000\n",
      "Processing image 3500/5000\n",
      "Processing image 3600/5000\n",
      "Processing image 3700/5000\n",
      "Processing image 3800/5000\n",
      "Processing image 3900/5000\n",
      "Processing image 4000/5000\n",
      "Processing image 4100/5000\n",
      "Processing image 4200/5000\n",
      "Processing image 4300/5000\n",
      "Processing image 4400/5000\n",
      "Processing image 4500/5000\n",
      "Processing image 4600/5000\n",
      "Processing image 4700/5000\n",
      "Processing image 4800/5000\n",
      "Processing image 4900/5000\n"
     ]
    }
   ],
   "source": [
    "results_v8 = run_inference(model_v8, image_ids)\n",
    "results_v11 = run_inference(model_v11, image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc24456-d3e1-4536-a662-f0eb9091b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total = 0   # Total number of ground truth objects\n",
    "N_loc = 0     # Objects localized by both models\n",
    "\n",
    "LNF_total = 0  # Location negative flips\n",
    "CNF_total = 0  # Classification negative flips \n",
    "BNF_total = 0  # Both negative flips \n",
    "TNF_total = 0  # Total Negative Flips (either location or classification )\n",
    "\n",
    "flip_details = []\n",
    "\n",
    "for img_id in (list(results_v8.keys())):\n",
    "    gt_objects = get_ground_truth_objects(img_id)\n",
    "    N_total += len(gt_objects)\n",
    "\n",
    "    detections_v8 = results_v8[img_id]\n",
    "    detections_v11 = results_v11[img_id]\n",
    "\n",
    "    matches_v8 = find_matching_detections(detections_v8, gt_objects, iou_threshold)\n",
    "    matches_v11 = find_matching_detections(detections_v11, gt_objects, iou_threshold)\n",
    "\n",
    "    for gt_idx, gt_obj in enumerate(gt_objects):\n",
    "        d1 = matches_v8[gt_idx]\n",
    "        d2 = matches_v11[gt_idx]\n",
    "        \n",
    "        # Location Negative Flip: LNF_{i,g} = 1 if d1 ≠ ∅ and d2 = ∅\n",
    "        LNF_i_g = 1 if (d1 is not None and d2 is None) else 0 \n",
    "\n",
    "        # Classification Negative Flip: CNF_{i,g} = 1 if both detected but v8 correct, v11 wrong\n",
    "        CNF_i_g = 0 \n",
    "        if d1 is not None and d2 is not None: \n",
    "            N_loc += 1 # Count objects localized by both models \n",
    "            if d1['class'] == gt_obj['class'] and d2['class'] != gt_obj['class']:\n",
    "                CNF_i_g = 1\n",
    "\n",
    "        BNF_i_g = LNF_i_g * CNF_i_g  \n",
    "        TNF_i_g = 1 if (LNF_i_g == 1 or CNF_i_g == 1) else 0\n",
    "\n",
    "        LNF_total += LNF_i_g\n",
    "        CNF_total += CNF_i_g\n",
    "        BNF_total += BNF_i_g\n",
    "        TNF_total += TNF_i_g\n",
    "\n",
    "        if LNF_i_g == 1 or CNF_i_g == 1:\n",
    "            flip_details.append({\n",
    "                'image_id': img_id,\n",
    "                'gt_class': gt_obj['class'],\n",
    "                'gt_bbox': gt_obj['bbox'],\n",
    "                'LNF': LNF_i_g,\n",
    "                'CNF': CNF_i_g,\n",
    "                'TNF': TNF_i_g,\n",
    "                'v8_detected': d1 is not None,\n",
    "                'v11_detected': d2 is not None,\n",
    "                'v8_class': d1['class'] if d1 else None,\n",
    "                'v11_class': d2['class'] if d2 else None,\n",
    "                'v8_confidence': d1['confidence'] if d1 else None,\n",
    "                'v11_confidence': d2['confidence'] if d2 else None\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1bb8f35-9f36-456b-9c74-863c756488c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LNF_rate = LNF_total / N_total if N_total > 0 else 0\n",
    "CNF_rate_standard = CNF_total / N_loc if N_loc > 0 else 0  # Standard: denominator = N_loc\n",
    "CNF_rate_common_denom = CNF_total / N_total if N_total > 0 else 0  # Common denominator for subtraction\n",
    "TNF_rate = TNF_total / N_total if N_total > 0 else 0\n",
    "\n",
    "# Compute the difference (using common denominator)\n",
    "flip_difference = CNF_rate_common_denom - LNF_rate\n",
    "\n",
    "results = {\n",
    "    'summary': {\n",
    "        'N_total': N_total,\n",
    "        'N_loc': N_loc,\n",
    "        'LNF_total': LNF_total,\n",
    "        'CNF_total': CNF_total,\n",
    "        'BNF_total': BNF_total,\n",
    "        'TNF_total': TNF_total,\n",
    "        'LNF_rate': LNF_rate,\n",
    "        'CNF_rate_standard': CNF_rate_standard,\n",
    "        'CNF_rate_common_denom': CNF_rate_common_denom,\n",
    "        'TNF_rate': TNF_rate,\n",
    "        'flip_difference': flip_difference,\n",
    "        'iou_threshold': iou_threshold\n",
    "    },\n",
    "    'flip_details': flip_details\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "with open('negative_flip_analysis_results.json', 'w') as f:\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    json_results = {\n",
    "        'summary': {k: (int(v) if isinstance(v, np.integer) else float(v) if isinstance(v, np.floating) else v) \n",
    "                   for k, v in results['summary'].items()},\n",
    "        'flip_details': results['flip_details']\n",
    "    }\n",
    "    json.dump(json_results, f, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba71ab2-e56b-49a3-8891-0ff69dc055d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N_total': 36335,\n",
       " 'N_loc': 17474,\n",
       " 'LNF_total': 1853,\n",
       " 'CNF_total': 92,\n",
       " 'BNF_total': 0,\n",
       " 'TNF_total': 1945,\n",
       " 'LNF_rate': 0.05099766065776799,\n",
       " 'CNF_rate_standard': 0.005264965090992332,\n",
       " 'CNF_rate_common_denom': 0.00253199394523187,\n",
       " 'TNF_rate': 0.05352965460299986,\n",
       " 'flip_difference': -0.048465666712536125,\n",
       " 'iou_threshold': 0.5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_results['summary']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
